"""
module to create PINN for solving an ODE
and function to use that PINN to solution
"""
import torch
import numpy as np, pandas as pd
import torch.nn as nn
from itertools import combinations_with_replacement
from typing import List, Callable, TypeAlias
import copy

TensorLike: TypeAlias = torch.Tensor | np.ndarray | pd.DataFrame
TensorListLike: TypeAlias = TensorLike | list[TensorLike] | tuple[TensorLike, ...]

def factorials_up_to_n(n :int) -> torch.Tensor:
    """
    returns a tensor of factorials
    from 0! to n!
    """
    if n < 0:
        raise ValueError("Factorials only have non-negative arguments")
    facs = torch.ones(n + 1, dtype=torch.float32)
    for k in range(1, n + 1):
        facs[k] = facs[k-1] * k
    return facs

def taylor_polynomial(a: float, ics: List[float]):
    """
    Generate Taylor polynomial centered at a
    of degree len(ics) - 1 using the ics to get coefficients
    """
    n = len(ics)
    facs = factorials_up_to_n(n)
    first_ic = ics[0]
    n_vars = 1 if isinstance(first_ic, (int, float)) else len(first_ic)
    def g(x):
        # make x at least 1D for broadcasting
        x = x.unsqueeze(-1) if x.ndim == 1 else x  # shape [batch,1] if batched
        result = torch.zeros(x.shape[0], n_vars, dtype=torch.float32)  # shape [batch, n_vars]
        for k, yk in enumerate(ics):
            yk_tensor = yk.clone().detach().float() if isinstance(yk, torch.Tensor) else torch.tensor(yk, dtype=torch.float32)
            result += (x - a)**k * (yk_tensor / facs[k])  # broadcasts over batch
        return result.squeeze(0) if result.shape[0] == 1 else result  # handle scalar x
    return g

def get_y_trial(a: float, ics: list, NN: nn.Module):
    """
    given a point a as a center,
    a list y(a), y'(a),... y^(n)(a)
    create trial solution that is the taylor polynomial
    generated by the ICs + (x-a)^n * NN(x-a)
    """
    n = len(ics)
    # get the taylor polynomial based on the ICs
    poly = taylor_polynomial(a, ics)
    
    def y_trial(x):
        # make x 2D: [batch, 1] for NN
        x_in = x.unsqueeze(-1) if x.ndim == 1 else x
        return poly(x_in) + (x_in - a)**n * NN(x_in - a)
    
    return y_trial


class PINN(nn.Module):
    """
    feed forward network intended to be used for solving DEs
    """
    def __init__(self,
                 num_hidden_layers:int = 2,
                 layer_width:int = 64,
                 input_activation: Callable[[torch.Tensor], torch.Tensor] = nn.Tanh(),
                 hidden_activation: Callable[[torch.Tensor], torch.Tensor] | List= nn.Tanh(),
                 output_activation: Callable[[torch.Tensor], torch.Tensor] = nn.Identity(),
                 num_inputs:int = 1,
                 num_outputs:int = 1
    ):
        """
        Parameters
        num_hidden_layers: int
            number of layers between output layer and input layer
        layer_width: int
            number of neurons in each layer
        input_activation : Callable 
            activation function to be used on input layer
        hidden_activation: Callable of List of Callables
            Activation function(s) for hidden layers
            If a single function is provided it is applied to all layers
            If a list is provided, it should have length equal to num_hidden_layers
        output_activation: Callable
            activation function for output layer
        num_inputs
            number of independent variables in DE
        num_outputs
            number of equations in a system of DEs
        """
        super(PINN, self).__init__()
        if not isinstance (hidden_activation, list):
            hidden_activation = [hidden_activation] * num_hidden_layers
        if len(hidden_activation) != num_hidden_layers:
            raise ValueError(
                f"Length of hidden_activation ({len(hidden_activation)}) "
                f"must match num_hidden_layers ({num_hidden_layers}). "
                f"Either pass a single activation or a list of {num_hidden_layers} activations."
                )
        self.input_activation = input_activation
        self.hidden_activation = hidden_activation
        self.input_layer = nn.Linear(num_inputs, layer_width)
        self.output_layer = nn.Linear(layer_width, num_outputs)
        self.hidden_layers = nn.ModuleList(
            [nn.Linear(layer_width, layer_width) for _ in range(num_hidden_layers)]
        )
        self.output_activation = output_activation
        

    def forward(self, x:torch.Tensor) -> torch.Tensor:
        x = self.input_activation(self.input_layer(x))
        for i,layer in enumerate(self.hidden_layers):
            x = self.hidden_activation[i](layer(x))
        x = self.output_layer(x)
        return self.output_activation(x)

def derivatives(y: torch.Tensor, x: torch.Tensor, order: int) -> List[torch.Tensor]:
    """
    function to return [y, y', y'', ... y^(order)]
    to be used in a differential equation F(x, y, y'..) = 0
    """
    derivs = [y]  # 0th derivative
    
    for _ in range(order):
        current = derivs[-1]
        
        # Handle multi-dimensional output
        if current.ndim > 1 and current.shape[1] > 1:
            # Compute derivative of each output component separately
            dy_dx_components = []
            for i in range(current.shape[1]):
                # compute dy_i/dx
                grad = torch.autograd.grad(
                    outputs=current[:, i],
                    inputs=x,
                    grad_outputs=torch.ones(current.shape[0], device=current.device),
                    create_graph=True,
                    retain_graph=True
                )[0]
                dy_dx_components.append(grad)
            # turn dy/dx into a tensor 
            dy_dx = torch.cat(dy_dx_components, dim=1)
        else:
            # Scalar case
            dy_dx = torch.autograd.grad(
                outputs=current,
                inputs=x,
                grad_outputs=torch.ones_like(current),
                create_graph=True
            )[0]
        
        derivs.append(dy_dx)
    
    return derivs  # [y, y', y'', ..., y^(order)]


def compute_unique_derivatives(NN, x, order=2):
    """
    Compute all unique derivatives of each NN output up to a given order.
    Mixed partials are treated as equal (∂²/∂x∂y = ∂²/∂y∂x).

    Args:
        NN: neural network mapping x -> y (can have multiple outputs)
        x: torch.Tensor of shape (batch_size, num_inputs), requires_grad=True
        order: max derivative order (integer)

    Returns:
        derivs_per_output: list of dicts, one per output
            Each dict maps derivative name -> tensor (batch_size,)
    """
    batch_size, num_inputs = x.shape
    y = NN(x)
    if y.ndim == 1:
        y = y.unsqueeze(-1)
    num_outputs = y.shape[1]

    derivs_per_output = []

    for j in range(num_outputs):
        yj = y[:, j]
        deriv_dict = {'y': yj}

        # Compute and store first-order derivatives
        first_order = [torch.autograd.grad(
            yj, x, grad_outputs=torch.ones_like(yj),
            create_graph=True, retain_graph=True
        )[0][:, i] for i in range(num_inputs)]
        for i, grad in enumerate(first_order):
            deriv_dict[f"x{i}"] = grad

        # Store for recursive construction
        prev_derivs = { (i,): first_order[i] for i in range(num_inputs) }

        # Higher-order derivatives
        for ord in range(2, order + 1):
            new_derivs = {}
            for combo in combinations_with_replacement(range(num_inputs), ord):
                shorter = combo[:-1]
                last = combo[-1]
                grad_prev = prev_derivs[shorter]
                grad = torch.autograd.grad(
                    grad_prev, x, grad_outputs=torch.ones_like(grad_prev),
                    create_graph=True, retain_graph=True
                )[0][:, last]
                name = "_".join(f"x{i}" for i in combo)
                deriv_dict[name] = grad
                new_derivs[combo] = grad
            prev_derivs = new_derivs

        derivs_per_output.append(deriv_dict)

    return derivs_per_output

def get_loss(a: float, ics: List[float], NN:nn.Module, F:Callable) ->Callable:
    """
    generating the loss function 
    based on F(x,y,y',...y^n) = 0
    """
    y_trial_fn = get_y_trial(a, ics, NN)
    def loss(x):
        y0 = y_trial_fn(x)
        derivs = derivatives(y0, x, len(ics))
        residual = F(x, *derivs)
        return torch.mean(residual**2)
    
    return loss

def process_training_data(X: TensorListLike) -> List[torch.Tensor]:
    # if a single type was passed, convert to a list 
    if not isinstance(X, (list, tuple)):
        X = [X]
    # convert all elements of X to torch tensors
    processed_X = []
    for x in X:
        if isinstance(x, pd.DataFrame):
            x = torch.tensor(x.values, dtype=torch.float32)
        elif isinstance(x, np.ndarray):
            x = torch.tensor(x, dtype=torch.float32)
        elif not torch.is_tensor(x):
            raise TypeError(f"Unsupported input type: {type(x)}")
        processed_X.append(x)
    return processed_X

def train(
          NN: nn.Module,
          X: TensorListLike,
          loss_fn : Callable,
          epochs: int = 5000,
          lr: float = 1e-3,
          batch_size: int | None= None,
          print_every: int = 500,
          use_val: bool = True,
          val_size: float | int = 0.2,
          early_stopping: int | None = None,
          return_checkpoints: bool = False,
          checkpoint_every: int = 20,
          progress_callback: Callable[[int, float, float], None] |None = None,
          progress_every: int = 1
          ):
    """
    trains a neural network with training data and loss function provided
    Parameters
    ----------
    NN : nn.Module
        a neural network of type nn.Module.  
    X : TensorListLike
        the training data
    loss_fn : Callable
        loss function to use during training.
    epochs : int
        number of epochs to train
    lr : float
        learning rate
    batch_size : int | None
        when not None, the size of batches to use during training
        otherwise training is done on the whole data set at once each epoch
    print_every: int
        How often to print out the current loss and validation loss
    use_val : bool
        whether or not to split into train/validation sets
    val_size: int | float
        what size to make the validation set. If this is an int,
        it is the absolute size of the validation set. If this is in (0,1),
        it is a relative size compared to the training set size
    early_stopping : dict | None
        When not None, keys for whether to stop early. Keys:
        min_epochs - minimum number of epochs before considering early stopping
        patience - how many epochs without improvement on val_loss needed before stopping
    return_checkpoints: bool
        whether or not to return intermediate solutions.
    checkpoint_every : int
        when return_checkpoints is true, how often to return a checkpoint of values
    progress_callback : Callable | None
        when not None, is a function used to send progress to whatever is calling solve
    progress_every : int
        when there is a progress_callback, how often to call it
    
    Returns
    -------
    checkpoints : List[Tuple(int,callable)] | None:
        when return_checkpoints is true a list of pairs (epoch, solution at epoch)
        otherwise returns None
    """
    
    # replace X with list of things that are all tensors
    X = process_training_data(X)
    # make sure all data sets have the same number of training points
    if use_val:
        batch_sizes = [x.shape[0] for x in X]
        if len(set(batch_sizes)) != 1:
            raise ValueError(f"All elements in X must have the same batch size, got {batch_sizes}")
        # get number of points being used
        data_size = X[0].shape[0]
        # make sure val_size is a valid input
        if isinstance(val_size, int):
            if val_size <=0 or val_size >= data_size:
                raise ValueError("Not a valid validation size")
        elif isinstance(val_size, float):
            if val_size <=0 or val_size >= 1:
                raise ValueError("Not a valid validation size")
            else:
                # when it's a float get the actual size of the validation set
                val_size = int(data_size * val_size)
        else:
            raise TypeError("Validation size must be int or float")
        # get rid of requires_grad on x if true so it can be split
        for i,x in enumerate(X):
            if x.requires_grad:
                X[i] = x.detach().clone().requires_grad_(False)
        # generate train/validation split based on val_size
        perm = torch.randperm(data_size)
        X_val = [x[perm[:val_size]].requires_grad_(True) for x in X]
        X_train = [x[perm[val_size:]].requires_grad_(True) for x in X]
        print(len(X_val), len(X_train))
    else:
        X_train =[x.requires_grad_(True) for x in X]
        X_val = None
    if return_checkpoints:
        checkpoints = []
    else:
        checkpoints = None
    train_size = X_train[0].shape[0]
    # when there's early stopping get min_epochs, patience
    # and set up best loss and epochs since best
    if early_stopping is not None:
        min_epochs = early_stopping.get('min_epochs', 20)
        patience = early_stopping.get('patience', 10)
        best_val_loss = float('inf')
        epochs_since_best = 0
        best_weights = None
    # initialize optimizer
    optimizer = torch.optim.Adam(params=NN.parameters(), lr=lr)
    # training loop
    for epoch in range(1, epochs + 1):
        # make sure NN is in training state
        NN.train()
        if batch_size is None:
            # when no batching exists reset optimizer
            # compute loss and do backpropogation
            optimizer.zero_grad()
            loss = loss_fn(*X_train)
            loss.backward()
            epoch_loss = loss.item()
            optimizer.step()
        else:
            # shuffle every epoch to get new batches
            perm = torch.randperm(train_size)
            epoch_loss = 0.0
            # loop over the batches and compute average loss
            # each batch run backpropogation
            for i in range(0, train_size, batch_size):
                optimizer.zero_grad()
                end = min(i+batch_size, train_size)
                idx = perm[i:end]
                X_batch = [x_train[idx] for x_train in X_train]
                loss = loss_fn(*X_batch)
                loss.backward()
                epoch_loss += loss.item() * len(idx)
                optimizer.step()
            epoch_loss /= train_size
        # compute loss on validation set
        # set NN to eval for this
        NN.eval()
        if use_val:
            val_loss = loss_fn(*X_val).item()
        else:
            val_loss = float('nan')
        # Call progress callback if provided (epoch, train_loss, val_loss)
        if progress_callback is not None:
            try:
                # Only invoke callback every `progress_every` epochs to reduce UI churn,
                # but always call on the final epoch so the UI can show completion.
                if progress_every <= 1 or (epoch % progress_every == 0) or (epoch == epochs):
                    progress_callback(epoch, float(epoch_loss), float(val_loss))
            except Exception:
                # Ensure progress callback failures don't stop training
                pass
        # check for early stopping conditions when they exist
        if use_val:
            if early_stopping is not None:
                if val_loss < best_val_loss:
                    best_val_loss = val_loss
                    epochs_since_best = 0
                    best_weights = {k: v.clone().detach() for k, v in NN.state_dict().items()}
                else:
                    epochs_since_best += 1
                if epochs_since_best >= patience and epoch >= min_epochs:
                    print(f"Early stopping at epoch {epoch}, validation loss did not improve for {early_stopping['patience']} epochs")
                    NN.load_state_dict(best_weights)
                    break
        # put back into training
        NN.train()
        if epoch % print_every == 0:
            print(f"Epoch {epoch}, Loss: {epoch_loss:.6f}, Validation Loss: {val_loss:.6f}")
        # When we are returning checkpoints, create the checkpoint function and add to list
        if return_checkpoints and epoch % checkpoint_every == 0:
            checkpoint_state = {k: v.clone() for k, v in NN.state_dict().items()}
            # create a function that uses a fresh NN with these weights
            nn_copy = copy.deepcopy(NN)       # or create a new PINN instance
            nn_copy.load_state_dict(checkpoint_state)
            nn_copy.eval()
            checkpoints.append((epoch, nn_copy))

    print(f"Final Epoch {epoch}, Loss: {epoch_loss:.6f}, Validation Loss: {val_loss:.6f}")
    # put into eval mode after training
    NN.eval()
    return checkpoints

def ode_solve(
        F : Callable,
        a : float,
        ics : List[float | torch.Tensor],
        X : torch.Tensor | List[torch.Tensor],
        NN : PINN,
        return_checkpoints : bool = False,
        **solve_args
):
    """
    Wrapper to call solve for an ODE
    Parameters
    ----------
    F : Callable
        F(x,y,y',...) which is the DE
    a: float
        x0 for ics
    ics: List [float or Tensor]
        list of y0, y_prime0, ...
    X : torch.Tensor or list[torch.Tensor]
        training data
    NN: PINN
        PINN to train to solve the DE
    return_checkpoints : bool
        whether or not to return intermediate solutions
    **solve_args:
        other arguments that can be passed directly to solve
    
    Returns
    -------
    solution: Callable
        the solution to the ODE that's y_trial(a, ics, NN)
    checkpoints (optional) : List[ tuple(int, callable
        when return_checkpoints is true, a list
        of pairs (epoch, trial) where trial is intermediate
        solution at epoch
    """

    loss_fn = get_loss(a, ics, NN, F)
    
    # Run the generic training loop
    result = train(
        NN=NN,
        X=X,
        loss_fn=loss_fn,
        return_checkpoints=return_checkpoints,
        **solve_args
    )

    # Wrap checkpoints so each stores only state_dict
    wrapped_checkpoints = None
    if return_checkpoints:
        checkpoints = result  # train returns the raw checkpoints
        wrapped_checkpoints = []
        for epoch, nn_checkpoint in checkpoints:
            # copy state_dict so we don't mutate the original network
            state = {k: v.clone() for k, v in nn_checkpoint.state_dict().items()}

            # closure capturing the state dict
            def trial(x, state=state):
                nn_copy = copy.deepcopy(NN)  # fresh network of same architecture
                nn_copy.load_state_dict(state)
                nn_copy.eval()
                with torch.no_grad():
                    return get_y_trial(a, ics, nn_copy)(x)

            wrapped_checkpoints.append((epoch, trial))

    solution = get_y_trial(a, ics, NN)
    if return_checkpoints:
        return solution, wrapped_checkpoints
    return solution

def get_pde_loss(NN, de_eq, ic_conditions=None, bc_conditions=None, DE_params=None):
    """
    Returns a loss function compatible with your `train()` loop.

    Parameters
    ----------
    NN : nn.Module
        The neural network model.
    de_eq : Callable[[nn.Module, torch.Tensor], torch.Tensor]
        PDE residual function (e.g. lambda NN, x: u_t - alpha * u_xx).
    ic_conditions : list of tuples | None
        Each tuple is (X_ic, u_ic, weight) where:
            X_ic : tensor of IC points
            u_ic : callable or tensor for target IC values
            weight : optional float (defaults to 1.0)
    bc_conditions : list of tuples | None
        Each tuple is (X_bc, u_bc, weight) with same semantics.
    DE_params : dict | None
        Optional PDE parameters (passed to `de_eq` if needed).

    Returns
    -------
    loss_fn : Callable
        A function that can be passed directly to `train()`.
    """

    # record what argument order this loss expects
    expected_keys = ["de"]
    if ic_conditions is not None:
        expected_keys.append("ic")
    if bc_conditions is not None:
        expected_keys.append("bc")

    def loss_fn(*X_sets):
        idx = 0
        total_loss = 0.0

        # PDE residual
        X_de = X_sets[idx]
        idx += 1
        de_residual = de_eq(NN, X_de) if DE_params is None else de_eq(NN, X_de, **DE_params)
        total_loss += torch.mean(de_residual**2)

        # Initial conditions
        if ic_conditions is not None:
            for (X_ic, u_ic, *maybe_weight) in ic_conditions:
                weight = maybe_weight[0] if maybe_weight else 1.0
                u_pred = NN(X_ic)
                u_true = u_ic(X_ic) if callable(u_ic) else u_ic
                # print(u_pred.shape, u_true.shape)
                total_loss += weight * torch.mean((u_pred - u_true)**2)
            idx += 1

        # Boundary conditions
        if bc_conditions is not None:
            for (X_bc, u_bc, *maybe_weight) in bc_conditions:
                weight = maybe_weight[0] if maybe_weight else 1.0
                u_pred = NN(X_bc)
                u_true = u_bc(X_bc) if callable(u_bc) else u_bc
                total_loss += weight * torch.mean((u_pred - u_true)**2)
            idx += 1

        return total_loss

    # attach expected_keys for consistency
    loss_fn.expected_keys = expected_keys
    return loss_fn