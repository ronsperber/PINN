"""
module to create PINN for solving an ODE
and function to use that PINN to solution
"""
import torch
import torch.nn as nn
from typing import List, Callable, Union, Optional
import copy

def factorials_up_to_n(n :int) -> torch.Tensor:
    """
    returns a tensor of factorials
    from 0! to n!
    """
    if n < 0:
        raise ValueError("Factorials only have non-negative arguments")
    facs = torch.ones(n + 1, dtype=torch.float32)
    for k in range(1, n + 1):
        facs[k] = facs[k-1] * k
    return facs

def taylor_polynomial(a: float, ics: List[float]):
    """
    Generate Taylor polynomial centered at a
    of degree len(ics) - 1 using the ics to get coefficients
    """
    n = len(ics)
    facs = factorials_up_to_n(n)
    first_ic = ics[0]
    n_vars = 1 if isinstance(first_ic, (int, float)) else len(first_ic)
    def g(x):
        # make x at least 1D for broadcasting
        x = x.unsqueeze(-1) if x.ndim == 1 else x  # shape [batch,1] if batched
        result = torch.zeros(x.shape[0], n_vars, dtype=torch.float32)  # shape [batch, n_vars]
        for k, yk in enumerate(ics):
            yk_tensor = yk.clone().detach().float() if isinstance(yk, torch.Tensor) else torch.tensor(yk, dtype=torch.float32)
            result += (x - a)**k * (yk_tensor / facs[k])  # broadcasts over batch
        return result.squeeze(0) if result.shape[0] == 1 else result  # handle scalar x
    return g

def get_y_trial(a: float, ics: list, NN: nn.Module):
    """
    given a point a as a center,
    a list y(a), y'(a),... y^(n)(a)
    create trial solution that is the taylor polynomial
    generated by the ICs + (x-a)^n * NN(x-a)
    """
    n = len(ics)
    # get the taylor polynomial based on the ICs
    poly = taylor_polynomial(a, ics)
    
    def y_trial(x):
        # make x 2D: [batch, 1] for NN
        x_in = x.unsqueeze(-1) if x.ndim == 1 else x
        return poly(x_in) + (x_in - a)**n * NN(x_in - a)
    
    return y_trial


class PINN(nn.Module):
    """
    feed forward network intended to be used for solving DEs
    """
    def __init__(self,
                 num_hidden_layers:int = 2,
                 layer_width:int = 64,
                 input_activation: Callable[[torch.Tensor], torch.tensor] = nn.Tanh(),
                 hidden_activation: Union[Callable[[torch.Tensor], torch.Tensor], List]= nn.Tanh(),
                 output_activation: Callable[[torch.Tensor], torch.Tensor] = nn.Identity(),
                 num_inputs:int = 1,
                 num_outputs:int = 1
    ):
        """
        Parameters
        num_hidden_layers: int
            number of layers between output layer and input layer
        layer_width: int
            number of neurons in each layer
        activation : Callable
            activation function to be used on layers
            other than the output layer
        output_activation: Callable
            activation function for output layer
        num_inputs
            number of independent variables in DE
        num_outputs
            number of equations in a system of DEs
        """
        super(PINN, self).__init__()
        if not isinstance (hidden_activation, list):
            hidden_activation = [hidden_activation] * num_hidden_layers
        self.input_activation = input_activation
        self.hidden_activation = hidden_activation
        self.input_layer = nn.Linear(num_inputs, layer_width)
        self.output_layer = nn.Linear(layer_width, num_outputs)
        self.hidden_layers = nn.ModuleList(
            [nn.Linear(layer_width, layer_width) for _ in range(num_hidden_layers)]
        )
        self.output_activation = output_activation
        

    def forward(self, x:torch.Tensor) -> torch.Tensor:
        x = self.input_activation(self.input_layer(x))
        for i,layer in enumerate(self.hidden_layers):
            x = self.hidden_activation[i](layer(x))
        x = self.output_layer(x)
        return self.output_activation(x)

def derivatives(y: torch.Tensor, x: torch.Tensor, order: int) -> List[torch.Tensor]:
    """
    function to return [y, y', y'', ... y^(order)]
    to be used in a differential equation F(x, y, y'..) = 0
    """
    derivs = [y]  # 0th derivative
    
    for _ in range(order):
        current = derivs[-1]
        
        # Handle multi-dimensional output
        if current.ndim > 1 and current.shape[1] > 1:
            # Compute derivative of each output component separately
            dy_dx_components = []
            for i in range(current.shape[1]):
                # compute dy_i/dx
                grad = torch.autograd.grad(
                    outputs=current[:, i],
                    inputs=x,
                    grad_outputs=torch.ones(current.shape[0], device=current.device),
                    create_graph=True,
                    retain_graph=True
                )[0]
                dy_dx_components.append(grad)
            # turn dy/dx into a tensor 
            dy_dx = torch.cat(dy_dx_components, dim=1)
        else:
            # Scalar case
            dy_dx = torch.autograd.grad(
                outputs=current,
                inputs=x,
                grad_outputs=torch.ones_like(current),
                create_graph=True
            )[0]
        
        derivs.append(dy_dx)
    
    return derivs  # [y, y', y'', ..., y^(order)]

def get_loss(a: float, ics: List[float], NN:nn.Module, F:Callable) ->Callable:
    """
    generating the loss function 
    based on F(x,y,y',...y^n) = 0
    """
    y_trial_fn = get_y_trial(a, ics, NN)
    def loss(x):
        y0 = y_trial_fn(x)
        derivs = derivatives(y0, x, len(ics))
        residual = F(x, *derivs)
        return torch.mean(residual**2)
    
    return loss

def solve(F: Callable,
          a : float,
          ics: List[float | torch.Tensor],
          NN: PINN,
          x: torch.Tensor,
          epochs: int = 5000,
          lr: float = 1e-3,
          batch_size: Optional[int]= None,
          print_every: int = 500,
          val_size: Union[float, int] = 0.2,
          early_stopping: Optional[dict] = None,
          return_checkpoints: bool = False,
          checkpoint_every: int = 20,
          progress_callback: Optional[Callable[[int, float, float], None]] = None,
          progress_every: int = 1
          ):
    """
    Use a PINN to solve an ODE
    Parameters
    ----------
    F : callable
        a function F(x,y,y',...y^(n)) that defines the
        DE by F(x,y,y',...y^(n)) = 0
    a: float
        the value of the independent variable at which
        the initial conditions are defined
    ics : List[float | torch.Tensor]
        the initial conditions y(a), y'(a),...
    NN : PINN
        a neural network of class PINN. 
        This is going to be trained to help approximate the solution
    x : torch.Tensor
        the values of x used to train the network
    epochs : int
        number of epochs to train
    lr : float
        learning rate
    batch size : Optional[int]
        when defined, the size of batches to use during training
        otherwise training is done on the whole data set at once each epoch
    print_every: int
        How often to print out the current loss and validation loss
    val_size: int | float
        what size to make the validation set. If this is an int
        it is the absolute size of the validation set. If this is in (0,1),
        it is a relative size compared to the training set size
    early_stopping : Optional[dict]
        When not none, keys for whether to stop early. Keys:
        min_epochs - minimum number of epochs before considering early stopping
        patience - how many epochs without improvement on val_loss needed before stopping
    return_checkpoints: bool
        whether or not to return intermediate solutions
    checkpoint_every : int
        when return_checkpoints is true, how often to return a checkpoint of values
    
    Returns
    solution : callable
        the function y = taylor_polynomial(a) + (x-a)**n * NN(x-a)
        that approximates the solution
    checkpoints (optional) : List[Tuple(int,callable)]
        when return_checkpoints is true a list of pairs (epoch, solution at epoch)
    """
    # get the loss function
    loss_fn = get_loss(a, ics, NN, F)
    # get number of points being used
    n_points = x.shape[0]
    # make sure val_size is a valid input
    if isinstance(val_size, int):
        if val_size <=0 or val_size >= n_points:
            raise ValueError("Not a valid validation size")
    elif isinstance(val_size, float):
        if val_size <=0 or val_size >= 1:
            raise ValueError("Not a valid validation size")
        else:
            # when it's a float get the actual size of the validation set
            val_size = int(n_points * val_size)
    else:
        raise TypeError("Validation size must be int or float")
    # get rid of requires_grad on x if true so it can be split
    if x.requires_grad:
        x = x.detach().clone().requires_grad_(False)
    # generate train/validation split based on val_size
    perm = torch.randperm(n_points)
    x_val = x[perm[:val_size]].requires_grad_(True)
    x_train = x[perm[val_size:]].requires_grad_(True)
    checkpoints = []
    n_train = x_train.shape[0]
    # when there's early stopping get min_epochs, patience
    # and set up best loss and epochs since best
    if early_stopping is not None:
        min_epochs = early_stopping.get('min_epochs', 20)
        patience = early_stopping.get('patience', 10)
        best_val_loss = float('inf')
        epochs_since_best = 0
        best_weights = None
    # initialize optimizer
    optimizer = torch.optim.Adam(params=NN.parameters(), lr=lr)
    # training loop
    for epoch in range(1, epochs + 1):
        # make sure NN is in training state
        NN.train()
        if batch_size is None:
            # when no batching exists reset optimizer
            # compute loss and do backpropogation
            optimizer.zero_grad()
            loss = loss_fn(x_train)
            loss.backward()
            epoch_loss = loss.item()
            optimizer.step()
        else:
            # shuffle every epoch to get new batches
            perm = torch.randperm(n_train)
            epoch_loss = 0.0
            # loop over the batches and compute average loss
            # each batch run backpropogation
            for i in range(0, n_train, batch_size):
                optimizer.zero_grad()
                end = min(i+batch_size, n_train)
                idx = perm[i:end]
                x_batch = x_train[idx]
                loss = loss_fn(x_batch)
                loss.backward()
                epoch_loss += loss.item() * len(idx)
                optimizer.step()
            epoch_loss /= n_train
        # compute loss on validation set
        # set NN to eval for this
        NN.eval()
        val_loss = loss_fn(x_val).item()
        # Call progress callback if provided (epoch, train_loss, val_loss)
        if progress_callback is not None:
            try:
                # Only invoke callback every `progress_every` epochs to reduce UI churn,
                # but always call on the final epoch so the UI can show completion.
                if progress_every <= 1 or (epoch % progress_every == 0) or (epoch == epochs):
                    progress_callback(epoch, float(epoch_loss), float(val_loss))
            except Exception:
                # Ensure progress callback failures don't stop training
                pass
        # check for early stopping conditions when they exist
        if early_stopping is not None:
            if val_loss < best_val_loss:
                best_val_loss = val_loss
                epochs_since_best = 0
                best_weights = {k: v.clone().detach() for k, v in NN.state_dict().items()}
            else:
                epochs_since_best += 1
            if epochs_since_best >= patience and epoch >= min_epochs:
                print(f"Early stopping at epoch {epoch}, validation loss did not improve for {early_stopping['patience']} epochs")
                NN.load_state_dict(best_weights)
                break
        # put back into training
        NN.train()
        if epoch % print_every == 0:
            print(f"Epoch {epoch}, Loss: {epoch_loss:.6f}, Validation Loss: {val_loss:.6f}")
        # When we are returning checkpoints, create the checkpoint function and add to list
        if return_checkpoints and epoch % checkpoint_every == 0:
            checkpoint_state = {k: v.clone() for k, v in NN.state_dict().items()}
            # create a function that uses a fresh NN with these weights
            def y_trial(x, state=checkpoint_state):
                # make a fresh NN instance
                nn_copy = copy.deepcopy(NN)       # or create a new PINN instance
                nn_copy.load_state_dict(state)
                nn_copy.eval()
                with torch.no_grad():
                    return get_y_trial(a, ics, nn_copy)(x)
            checkpoints.append((epoch, y_trial))

    print(f"Final Epoch {epoch}, Loss: {epoch_loss:.6f}, Validation Loss: {val_loss:.6f}")
    # put into eval mode after training
    NN.eval()
    # get the solution, but use it in no_grad mode to save grad calcs
    y_trial_grad = get_y_trial(a, ics, NN)
    def y_trial(x):
        with torch.no_grad():
            return y_trial_grad(x)
    if return_checkpoints:
        return y_trial, checkpoints
    return y_trial



